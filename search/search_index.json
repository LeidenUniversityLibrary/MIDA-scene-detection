{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This repository has scripts that help you: detect scene (i.e. shot) changes convert ffmpeg \"flat format\" output to CSV files run OCR on videos with Arabic subtitles convert timestamps from hours:minutes:seconds to seconds align approximate timestamps to detected scenes generate scripts to extract all frames from selected scenes as images Detect scene (i.e. shot) changes \u00b6 Use the scdet filter from ffmpeg. The scdet.sh script wraps the command below and outputs data in the flat format that can be converted to CSV with flat-to-csv.sh . cd episodes/ $EPISODE ${ THIS_REPO } /scdet.sh data/video.ext # The command and output file name are printed ${ THIS_REPO } /flat-to-csv.sh ${ OUTPUT_FILE_NAME } Note: Some shots fade-cross instead of using hard cuts. How should those be handled? cd episodes/ $EPISODE # Extract the first frame of each new scene ffmpeg -report -i data/video.webm -filter_complex \"blackframe=amount=99:threshold=24,scdet=threshold=6.0,metadata=select:key=lavfi.scd.time,metadata=print:file='data_out/frames.txt'\" -vsync 2 \"data_out/scene-%03d.jpg\" # Analyse each frame and determine its blackness and the score for scene change # Print timestamp, height and width information about the frames and all filter-added tags ffprobe -report -f lavfi -i \"movie=data/kf5USsE6Xq8.webm,blackframe=amount=99:threshold=24,scdet=threshold=5.0\" -show_entries \"frame=pkt_pts_time,height,width:frame_tags\" -print_format flat > data_out/scene-changes.txt Run OCR to extract subtitles from frames \u00b6 Previous attempts have been documented in the OCR docs . The most current commands have been listed in the ocr-run.sh script. Convert ffmpeg \"flat output\" to CSV \u00b6 Use the flat-to-csv.sh script: ./flat-to-csv.sh INPUT_FILE [ OUTPUT_FILE ] Getting source material \u00b6 These tools work on downloaded video files. This should be allowed for research at academic institutions by European copyright directive; in the Netherlands this is allowed under Auteurswet article 15n. On the command line, we can use youtube-dl to download specific videos or (parts of) playlists. Youtube-dl is available via Homebrew: brew install youtube-dl # Download videos in a YouTube playlist youtube-dl --all-subs -o '%(playlist)s/%(title)s/data/%(id)s.%(ext)s' --playlist-reverse --playlist-start 132 --playlist-end 133 --write-description --write-info-json --write-annotations -w 'https://www.youtube.com/playlist?list=PLge_kMuGwvL8pfyOpLP0hXprI7yQ0L2q-' # Download a specific video youtube-dl --all-subs --write-description --write-info-json --write-annotations -w 'https://www.youtube.com/watch?v=ZK9sYnqO2TI'","title":"Overview"},{"location":"#detect-scene-ie-shot-changes","text":"Use the scdet filter from ffmpeg. The scdet.sh script wraps the command below and outputs data in the flat format that can be converted to CSV with flat-to-csv.sh . cd episodes/ $EPISODE ${ THIS_REPO } /scdet.sh data/video.ext # The command and output file name are printed ${ THIS_REPO } /flat-to-csv.sh ${ OUTPUT_FILE_NAME } Note: Some shots fade-cross instead of using hard cuts. How should those be handled? cd episodes/ $EPISODE # Extract the first frame of each new scene ffmpeg -report -i data/video.webm -filter_complex \"blackframe=amount=99:threshold=24,scdet=threshold=6.0,metadata=select:key=lavfi.scd.time,metadata=print:file='data_out/frames.txt'\" -vsync 2 \"data_out/scene-%03d.jpg\" # Analyse each frame and determine its blackness and the score for scene change # Print timestamp, height and width information about the frames and all filter-added tags ffprobe -report -f lavfi -i \"movie=data/kf5USsE6Xq8.webm,blackframe=amount=99:threshold=24,scdet=threshold=5.0\" -show_entries \"frame=pkt_pts_time,height,width:frame_tags\" -print_format flat > data_out/scene-changes.txt","title":"Detect scene (i.e. shot) changes"},{"location":"#run-ocr-to-extract-subtitles-from-frames","text":"Previous attempts have been documented in the OCR docs . The most current commands have been listed in the ocr-run.sh script.","title":"Run OCR to extract subtitles from frames"},{"location":"#convert-ffmpeg-flat-output-to-csv","text":"Use the flat-to-csv.sh script: ./flat-to-csv.sh INPUT_FILE [ OUTPUT_FILE ]","title":"Convert ffmpeg \"flat output\" to CSV"},{"location":"#getting-source-material","text":"These tools work on downloaded video files. This should be allowed for research at academic institutions by European copyright directive; in the Netherlands this is allowed under Auteurswet article 15n. On the command line, we can use youtube-dl to download specific videos or (parts of) playlists. Youtube-dl is available via Homebrew: brew install youtube-dl # Download videos in a YouTube playlist youtube-dl --all-subs -o '%(playlist)s/%(title)s/data/%(id)s.%(ext)s' --playlist-reverse --playlist-start 132 --playlist-end 133 --write-description --write-info-json --write-annotations -w 'https://www.youtube.com/playlist?list=PLge_kMuGwvL8pfyOpLP0hXprI7yQ0L2q-' # Download a specific video youtube-dl --all-subs --write-description --write-info-json --write-annotations -w 'https://www.youtube.com/watch?v=ZK9sYnqO2TI'","title":"Getting source material"},{"location":"annotation/","text":"Course-grained annotations \u00b6 Going from approximate timestamps to scene-aligned annotations Fine-grained annotations \u00b6 Going from precise timestamps to exact frame annotations","title":"Annotating AV materials"},{"location":"annotation/#course-grained-annotations","text":"Going from approximate timestamps to scene-aligned annotations","title":"Course-grained annotations"},{"location":"annotation/#fine-grained-annotations","text":"Going from precise timestamps to exact frame annotations","title":"Fine-grained annotations"},{"location":"deprecated/","text":"While we are trying to optimise workflows, we cannot say goodbye to our first attempts just yet. Some of them are listed below. Convert ffmpeg frames.txt to CSV (deprecated) \u00b6 This is no longer necessary, since ffmpeg is set to output in its \"flat format\". awk -f scripts/frames-to-csv.awk episodes/1/data_out/frames.txt > episodes/1/data_manual/scenes.csv Select rows from CSV (deprecated) \u00b6 Using csvkit , I can find the rows that have been marked to contain the Star of David. This query works for small files, in which I manually aligned the manual timestamps with the scene times. csvsql --query 'select * from scenes_david where david is not NULL' -v data_manual/scenes_david.csv Merge manual marks with scenes (deprecated) \u00b6 The manual marks indicate rough timestamps at which a Star of David is visible. I want to link these timestamps to detected scenes so that I can extract frames from the whole scene, to help provide training data. cd episodes/ $EPISODE csvsql --query 'select s.*, m.* from scenes s, david1 m where s.start_ms <= m.ms and s.end_ms >= m.ms' \\ -v data_manual/scenes.csv data_manual/david1.csv > data_manual/merged_marks.csv There is an error in my assumption that I need only the scenes in which the 123000 millisecond falls. Instead I need all scenes that cover [123000,124000) . cd episodes/ $EPISODE csvsql --query 'select s.*, m.s from scenes s, david2 m where (s.start_ms <= m.ms and s.end_ms >= m.ms) or (s.start_ms <= m.ms + 999 and s.end_ms >= m.ms + 999)' \\ -v data_manual/scenes.csv data_manual/david2.csv > data_manual/merged_marks2.csv In ELAN, I checked which of the adjourning scenes actually contain the symbol. I exported these annotations as TSV with too many columns (and no header, as ELAN doesn't do headers). Therefore I need to select the columns I need, in the correct order: csvcut -t -H -c 12 ,4,7 data_manual/2-annotations.tsv > data_manual/adjusted_marks.csv I then add a header to this file and create the script to extract scenes. Extract scenes (deprecated old way) \u00b6 cd episodes/1/ ffmpeg -report -ss 165927ms -to 167087ms -i data/video.webm -vsync 2 data_out/scene-0066/frame-%03d.jpg Create a script that extracts each scene as in the above command: (note that you have to set the video file name yourself) cd episodes/ $EPISODE awk -f ../../scripts/marks-to-sh.awk video = 4QIcBbbPojc.mkv data_manual/merged_marks.csv > extract.sh Check for duplicate commands in extract.sh , as sometimes multiple manually identified timestamps may be in the same scene. The results would the same regardless, but it's wasted time.","title":"Deprecated scripts and commands"},{"location":"deprecated/#convert-ffmpeg-framestxt-to-csv-deprecated","text":"This is no longer necessary, since ffmpeg is set to output in its \"flat format\". awk -f scripts/frames-to-csv.awk episodes/1/data_out/frames.txt > episodes/1/data_manual/scenes.csv","title":"Convert ffmpeg frames.txt to CSV (deprecated)"},{"location":"deprecated/#select-rows-from-csv-deprecated","text":"Using csvkit , I can find the rows that have been marked to contain the Star of David. This query works for small files, in which I manually aligned the manual timestamps with the scene times. csvsql --query 'select * from scenes_david where david is not NULL' -v data_manual/scenes_david.csv","title":"Select rows from CSV (deprecated)"},{"location":"deprecated/#merge-manual-marks-with-scenes-deprecated","text":"The manual marks indicate rough timestamps at which a Star of David is visible. I want to link these timestamps to detected scenes so that I can extract frames from the whole scene, to help provide training data. cd episodes/ $EPISODE csvsql --query 'select s.*, m.* from scenes s, david1 m where s.start_ms <= m.ms and s.end_ms >= m.ms' \\ -v data_manual/scenes.csv data_manual/david1.csv > data_manual/merged_marks.csv There is an error in my assumption that I need only the scenes in which the 123000 millisecond falls. Instead I need all scenes that cover [123000,124000) . cd episodes/ $EPISODE csvsql --query 'select s.*, m.s from scenes s, david2 m where (s.start_ms <= m.ms and s.end_ms >= m.ms) or (s.start_ms <= m.ms + 999 and s.end_ms >= m.ms + 999)' \\ -v data_manual/scenes.csv data_manual/david2.csv > data_manual/merged_marks2.csv In ELAN, I checked which of the adjourning scenes actually contain the symbol. I exported these annotations as TSV with too many columns (and no header, as ELAN doesn't do headers). Therefore I need to select the columns I need, in the correct order: csvcut -t -H -c 12 ,4,7 data_manual/2-annotations.tsv > data_manual/adjusted_marks.csv I then add a header to this file and create the script to extract scenes.","title":"Merge manual marks with scenes (deprecated)"},{"location":"deprecated/#extract-scenes-deprecated-old-way","text":"cd episodes/1/ ffmpeg -report -ss 165927ms -to 167087ms -i data/video.webm -vsync 2 data_out/scene-0066/frame-%03d.jpg Create a script that extracts each scene as in the above command: (note that you have to set the video file name yourself) cd episodes/ $EPISODE awk -f ../../scripts/marks-to-sh.awk video = 4QIcBbbPojc.mkv data_manual/merged_marks.csv > extract.sh Check for duplicate commands in extract.sh , as sometimes multiple manually identified timestamps may be in the same scene. The results would the same regardless, but it's wasted time.","title":"Extract scenes (deprecated old way)"},{"location":"extract-frames/","text":"The computer vision models are trained on video frames. Therefore we have to extract frames from the source video files. Limits on files per directory \u00b6 It's better for many systems to not have too many files in a single directory. ALICE documentation mentions 100s, not 1000s as a rule of thumb. Our scripts try to work with these limitations. Using Python and OpenCV \u00b6 This appears to be slower. Using Bash and ffmpeg \u00b6 This appears to be faster.","title":"Extract frames from videos"},{"location":"extract-frames/#limits-on-files-per-directory","text":"It's better for many systems to not have too many files in a single directory. ALICE documentation mentions 100s, not 1000s as a rule of thumb. Our scripts try to work with these limitations.","title":"Limits on files per directory"},{"location":"extract-frames/#using-python-and-opencv","text":"This appears to be slower.","title":"Using Python and OpenCV"},{"location":"extract-frames/#using-bash-and-ffmpeg","text":"This appears to be faster.","title":"Using Bash and ffmpeg"},{"location":"ocr/","text":"The ocr.sh script takes a video file as input that is binarised and has only black text on a white background. Run the ffmpeg OCR filter with Arabic as selected language. By default, ffmpeg only allows ASCII characters to be recognised. We can override the default by setting whitelist='' as an option for the OCR filter. Do not output the media streams. cd episodes/ $EPISODE ffmpeg -report -i data/video.webm -filter_complex \"ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -f null out.null ffmpeg -report -i data/video.webm -filter_complex \"crop=in_w:in_h*0.35:0:in_h*0.65,lumakey=0.48:0.39,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -f null out.null ffmpeg -report -i data/episode3-ar-subs.mp4 -filter_complex \"crop=in_w:in_h*0.35:0:in_h*0.65,lumakey=0.48:0.39,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -f null out.null Find a good crop with ffplay \u00b6 ffplay -ss 300000ms -t 10000ms -vf \"edgedetect=mode=colorize,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"colorhold=color=white:similarity=0.01,edgedetect=mode=colormix,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"colorhold=color=white:similarity=0.01\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.35:0:in_h*0.65,colorhold=color=white:similarity=0.01\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.35:0:in_h*0.65,limiter=min=60000,colorhold=color=white:similarity=0.01\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.35:0:in_h*0.65,lumakey=0.5:0.4\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.35:0:in_h*0.65,lumakey=0.48:0.39,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data/episode3-ar-subs.mp4 ffplay -report -ss 300000ms -vf \"bbox=min_val=100,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data/episode3-ar-subs.mp4 ffplay -report -ss 300000ms -vf \"lutyuv=u=128:v=128,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data/episode3-ar-subs.mp4 Find parameters for creating black text on white background \u00b6 ffmpeg -report -f lavfi -i testsrc = s = 1280x720 -f lavfi -i color = gray:s = 1280x720 -f lavfi -i color = white:s = 1280x720 -f lavfi -i color = black:s = 1280x720 -lavfi threshold output.avi ffmpeg -report -ss 300000ms -i data/episode3-ar-subs.mp4 -f lavfi -i color = gray:s = 1280x720 -f lavfi -i color = white:s = 1280x720 -f lavfi -i color = black:s = 1280x720 -lavfi threshold data_out/3-binarised.mp4 ffmpeg -report -ss 300000ms -i data/episode3-ar-subs.mp4 -f lavfi -i \"color=#EEEEEE:s=1280x720\" -f lavfi -i color = white:s = 1280x720 -f lavfi -i color = black:s = 1280x720 -lavfi threshold data_out/3-binarised-eeeeee.mp4 ffmpeg -report -ss 300000ms -t 10000ms -i data/episode3-ar-subs.mp4 -f lavfi -i \"color=#DDDDDD:s=1280x720\" -f lavfi -i color = white:s = 1280x720 -f lavfi -i color = black:s = 1280x720 -lavfi threshold -an data_out/3-binarised-dddddd.mp4 ffmpeg -report -i data/episode3-ar-subs.mp4 -f lavfi -i \"color=#DDDDDD:s=1280x720\" -f lavfi -i color = white:s = 1280x720 -f lavfi -i color = black:s = 1280x720 -lavfi threshold -an -r 25 data_out/3-binarised-dddddd.mp4 Adjust cropping even more \u00b6 ffplay -vf \"crop=in_w:in_h*0.35:0:in_h*0.65,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data_out/3-binarised-dddddd.mp4 ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.35:0:in_h*0.65\" -i data_out/3-binarised-dddddd.mp4 # More optimal settings: 26% height starting 71% from the top of the frame ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.26:0:in_h*0.71\" -i data_out/3-binarised-dddddd.mp4 # Scale the cropped image ffplay -ss 350000ms -vf \"crop=in_w:in_h*0.26:0:in_h*0.71,scale=w=iw/2:h=ih/2\" -i data_out/3-binarised-dddddd.mp4 Output OCR results directly with ffprobe \u00b6 ffprobe -report -f lavfi -i movie = data_out/3-binarised-dddddd.mp4,crop = in_w:in_h*0.35:0:in_h*0.65,signalstats,ocr = language = ara:whitelist = '' ,metadata = print:file = 'data_out/ocr-cropped.txt' -show_entries frame = pkt_pts_time:frame_tags = lavfi.signalstats.YMIN,lavfi.signalstats.YLOW,lavfi.signalstats.YAVG,lavfi.signalstats.YHIGH,lavfi.signalstats.YMAX,lavfi.ocr.text,lavfi.ocr.confidence -print_format csv > data_out/3-dddddd-probe-ocr.csv ffprobe -report -f lavfi -i movie = data_out/3-binarised-dddddd.mp4,crop = in_w:in_h*0.35:0:in_h*0.65,scale = w = iw/2:h = ih/2,ocr = language = ara:whitelist = '' ,scdet = threshold = 6 .0 -show_entries frame = pkt_pts_time:frame_tags = lavfi.scd.mafd,lavfi.scd.score,lavfi.ocr.text,lavfi.ocr.confidence -print_format csv > data_out/3-dddddd-probe-scaled-ocr.csv ffprobe -report -f lavfi -i movie = data_out/3-binarised-dddddd.mp4,crop = in_w:in_h*0.35:0:in_h*0.65,scale = w = iw/2:h = ih/2,ocr = language = ara:whitelist = '' ,scdet = threshold = 6 .0 -show_entries frame = pkt_pts_time:frame_tags = lavfi.scd.mafd,lavfi.scd.score,lavfi.ocr.text,lavfi.ocr.confidence -print_format csv > data_out/3-dddddd-probe-scaled-ocr.csv","title":"Run OCR to extract (Arabic) subtitles from frames"},{"location":"ocr/#find-a-good-crop-with-ffplay","text":"ffplay -ss 300000ms -t 10000ms -vf \"edgedetect=mode=colorize,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"colorhold=color=white:similarity=0.01,edgedetect=mode=colormix,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"colorhold=color=white:similarity=0.01\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.35:0:in_h*0.65,colorhold=color=white:similarity=0.01\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.35:0:in_h*0.65,limiter=min=60000,colorhold=color=white:similarity=0.01\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.35:0:in_h*0.65,lumakey=0.5:0.4\" -i data/episode3-ar-subs.mp4 ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.35:0:in_h*0.65,lumakey=0.48:0.39,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data/episode3-ar-subs.mp4 ffplay -report -ss 300000ms -vf \"bbox=min_val=100,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data/episode3-ar-subs.mp4 ffplay -report -ss 300000ms -vf \"lutyuv=u=128:v=128,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data/episode3-ar-subs.mp4","title":"Find a good crop with ffplay"},{"location":"ocr/#find-parameters-for-creating-black-text-on-white-background","text":"ffmpeg -report -f lavfi -i testsrc = s = 1280x720 -f lavfi -i color = gray:s = 1280x720 -f lavfi -i color = white:s = 1280x720 -f lavfi -i color = black:s = 1280x720 -lavfi threshold output.avi ffmpeg -report -ss 300000ms -i data/episode3-ar-subs.mp4 -f lavfi -i color = gray:s = 1280x720 -f lavfi -i color = white:s = 1280x720 -f lavfi -i color = black:s = 1280x720 -lavfi threshold data_out/3-binarised.mp4 ffmpeg -report -ss 300000ms -i data/episode3-ar-subs.mp4 -f lavfi -i \"color=#EEEEEE:s=1280x720\" -f lavfi -i color = white:s = 1280x720 -f lavfi -i color = black:s = 1280x720 -lavfi threshold data_out/3-binarised-eeeeee.mp4 ffmpeg -report -ss 300000ms -t 10000ms -i data/episode3-ar-subs.mp4 -f lavfi -i \"color=#DDDDDD:s=1280x720\" -f lavfi -i color = white:s = 1280x720 -f lavfi -i color = black:s = 1280x720 -lavfi threshold -an data_out/3-binarised-dddddd.mp4 ffmpeg -report -i data/episode3-ar-subs.mp4 -f lavfi -i \"color=#DDDDDD:s=1280x720\" -f lavfi -i color = white:s = 1280x720 -f lavfi -i color = black:s = 1280x720 -lavfi threshold -an -r 25 data_out/3-binarised-dddddd.mp4","title":"Find parameters for creating black text on white background"},{"location":"ocr/#adjust-cropping-even-more","text":"ffplay -vf \"crop=in_w:in_h*0.35:0:in_h*0.65,ocr=language=ara:whitelist='',metadata=print:file='data_out/ocr.txt'\" -i data_out/3-binarised-dddddd.mp4 ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.35:0:in_h*0.65\" -i data_out/3-binarised-dddddd.mp4 # More optimal settings: 26% height starting 71% from the top of the frame ffplay -ss 300000ms -vf \"crop=in_w:in_h*0.26:0:in_h*0.71\" -i data_out/3-binarised-dddddd.mp4 # Scale the cropped image ffplay -ss 350000ms -vf \"crop=in_w:in_h*0.26:0:in_h*0.71,scale=w=iw/2:h=ih/2\" -i data_out/3-binarised-dddddd.mp4","title":"Adjust cropping even more"},{"location":"ocr/#output-ocr-results-directly-with-ffprobe","text":"ffprobe -report -f lavfi -i movie = data_out/3-binarised-dddddd.mp4,crop = in_w:in_h*0.35:0:in_h*0.65,signalstats,ocr = language = ara:whitelist = '' ,metadata = print:file = 'data_out/ocr-cropped.txt' -show_entries frame = pkt_pts_time:frame_tags = lavfi.signalstats.YMIN,lavfi.signalstats.YLOW,lavfi.signalstats.YAVG,lavfi.signalstats.YHIGH,lavfi.signalstats.YMAX,lavfi.ocr.text,lavfi.ocr.confidence -print_format csv > data_out/3-dddddd-probe-ocr.csv ffprobe -report -f lavfi -i movie = data_out/3-binarised-dddddd.mp4,crop = in_w:in_h*0.35:0:in_h*0.65,scale = w = iw/2:h = ih/2,ocr = language = ara:whitelist = '' ,scdet = threshold = 6 .0 -show_entries frame = pkt_pts_time:frame_tags = lavfi.scd.mafd,lavfi.scd.score,lavfi.ocr.text,lavfi.ocr.confidence -print_format csv > data_out/3-dddddd-probe-scaled-ocr.csv ffprobe -report -f lavfi -i movie = data_out/3-binarised-dddddd.mp4,crop = in_w:in_h*0.35:0:in_h*0.65,scale = w = iw/2:h = ih/2,ocr = language = ara:whitelist = '' ,scdet = threshold = 6 .0 -show_entries frame = pkt_pts_time:frame_tags = lavfi.scd.mafd,lavfi.scd.score,lavfi.ocr.text,lavfi.ocr.confidence -print_format csv > data_out/3-dddddd-probe-scaled-ocr.csv","title":"Output OCR results directly with ffprobe"},{"location":"scene-detect/","text":"Detect scene (i.e. shot) changes \u00b6 Use the scdet filter from ffmpeg. The scdet.sh script wraps the command below and outputs data in the flat format that can be converted to CSV with flat-to-csv.sh . cd episodes/ $EPISODE ${ THIS_REPO } /scdet.sh data/video.ext # The command and output file name are printed ${ THIS_REPO } /flat-to-csv.sh ${ OUTPUT_FILE_NAME } Note: Some shots fade-cross instead of using hard cuts. How should those be handled? cd episodes/ $EPISODE # Extract the first frame of each new scene ffmpeg -report -i data/video.webm -filter_complex \"blackframe=amount=99:threshold=24,scdet=threshold=6.0,metadata=select:key=lavfi.scd.time,metadata=print:file='data_out/frames.txt'\" -vsync 2 \"data_out/scene-%03d.jpg\" # Analyse each frame and determine its blackness and the score for scene change # Print timestamp, height and width information about the frames and all filter-added tags ffprobe -report -f lavfi -i \"movie=data/kf5USsE6Xq8.webm,blackframe=amount=99:threshold=24,scdet=threshold=5.0\" -show_entries \"frame=pkt_pts_time,height,width:frame_tags\" -print_format flat > data_out/scene-changes.txt Scene detection on ALICE \u00b6 The MIDA video analysis repository contains a SLURM job to run scene detection on all episodes and save the results to SURFdrive. These raw results are saved in zip files. To process these raw results into a table of scenes and a table of frames with scene numbers, change into the directory with the zip files and run: for E in { 2 ..54 } ; do F = $( ls ep ${ E } _*.zip ) mkdir -p ${ E } unzip ${ F } \"ep ${ E } _ffprobe-flat.txt\" -d ${ E } python ~/git/MIDA-scene-detection/scripts/pivot.py -m -o ${ E } /frames.csv ${ E } /*_ffprobe-flat.txt python ~/git/MIDA-scene-detection/scripts/mark_scenes.py --output-frames ${ E } /frames-with-scenes.csv --output-scenes ${ E } /scenes.csv ${ E } /frames.csv rm ${ E } /*.txt ${ E } /frames.csv done","title":"Scene detection"},{"location":"scene-detect/#detect-scene-ie-shot-changes","text":"Use the scdet filter from ffmpeg. The scdet.sh script wraps the command below and outputs data in the flat format that can be converted to CSV with flat-to-csv.sh . cd episodes/ $EPISODE ${ THIS_REPO } /scdet.sh data/video.ext # The command and output file name are printed ${ THIS_REPO } /flat-to-csv.sh ${ OUTPUT_FILE_NAME } Note: Some shots fade-cross instead of using hard cuts. How should those be handled? cd episodes/ $EPISODE # Extract the first frame of each new scene ffmpeg -report -i data/video.webm -filter_complex \"blackframe=amount=99:threshold=24,scdet=threshold=6.0,metadata=select:key=lavfi.scd.time,metadata=print:file='data_out/frames.txt'\" -vsync 2 \"data_out/scene-%03d.jpg\" # Analyse each frame and determine its blackness and the score for scene change # Print timestamp, height and width information about the frames and all filter-added tags ffprobe -report -f lavfi -i \"movie=data/kf5USsE6Xq8.webm,blackframe=amount=99:threshold=24,scdet=threshold=5.0\" -show_entries \"frame=pkt_pts_time,height,width:frame_tags\" -print_format flat > data_out/scene-changes.txt","title":"Detect scene (i.e. shot) changes"},{"location":"scene-detect/#scene-detection-on-alice","text":"The MIDA video analysis repository contains a SLURM job to run scene detection on all episodes and save the results to SURFdrive. These raw results are saved in zip files. To process these raw results into a table of scenes and a table of frames with scene numbers, change into the directory with the zip files and run: for E in { 2 ..54 } ; do F = $( ls ep ${ E } _*.zip ) mkdir -p ${ E } unzip ${ F } \"ep ${ E } _ffprobe-flat.txt\" -d ${ E } python ~/git/MIDA-scene-detection/scripts/pivot.py -m -o ${ E } /frames.csv ${ E } /*_ffprobe-flat.txt python ~/git/MIDA-scene-detection/scripts/mark_scenes.py --output-frames ${ E } /frames-with-scenes.csv --output-scenes ${ E } /scenes.csv ${ E } /frames.csv rm ${ E } /*.txt ${ E } /frames.csv done","title":"Scene detection on ALICE"}]}